import os
from openai import OpenAI
from dotenv import load_dotenv
import fitz  # PyMuPDF
from docx import Document

load_dotenv()

MAX_CHARS = 20000
COLLECTION_NAME = os.getenv("COLLECTION_NAME")

def process_documents(folder_path):
    results = []
    file_id = 1

    for file_name in os.listdir(folder_path):
        path = os.path.join(folder_path, file_name)
        ext = os.path.splitext(file_name)[1].lower()

        try:
            if ext == ".pdf":
                print(f"üìÑ Extracting PDF: {file_name}")
                chunks = extract_chunks_from_pdf(path)
            elif ext == ".docx":
                print(f"üìù Extracting DOCX: {file_name}")
                chunks = extract_chunks_from_docx(path)
            else:
                print(f"‚ö†Ô∏è Skipping unsupported file: {file_name}")
                continue
        except Exception as e:
            print(f"‚ùå Error extracting from {file_name}: {e}")
            continue

        for chunk in chunks:
            embedding = embed_text(chunk)
            if embedding:
                results.append({
                    "id": file_id,
                    "text": chunk,
                    "embedding": embedding,
                    "source": file_name
                })
                file_id += 1
            else:
                print(f"‚ùå Embedding failed for chunk from {file_name}")

    print(f"‚úÖ Total valid chunks processed: {len(results)}")
    return results

def extract_chunks_from_pdf(file_path):
    try:
        doc = fitz.open(file_path)
        chunks = []
        for page in doc:
            text = page.get_text()
            while text:
                chunks.append(text[:MAX_CHARS].strip())
                text = text[MAX_CHARS:]
        return chunks
    except Exception as e:
        print(f"‚ùå PDF read error: {e}")
        return []

def extract_chunks_from_docx(file_path):
    try:
        doc = Document(file_path)
        full_text = "\n".join(p.text for p in doc.paragraphs if p.text.strip())
        chunks = []
        while full_text:
            chunks.append(full_text[:MAX_CHARS].strip())
            full_text = full_text[MAX_CHARS:]
        return chunks
    except Exception as e:
        print(f"‚ùå DOCX read error: {e}")
        return []

def embed_text(text):
    try:
        client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        response = client.embeddings.create(
            model="text-embedding-3-large",
            input=text
        )
        embedding = response.data[0].embedding
        if not embedding or len(embedding) != 3072:
            print("‚ö†Ô∏è Invalid embedding length:", len(embedding))
        return embedding
    except Exception as e:
        print("‚ùå Error during embedding:", str(e))
        return None

def upsert_documents(client, docs):
    print(f"üÜô Starting upsert to collection: {COLLECTION_NAME}")
    valid_data = []

    for doc in docs:
        vec = doc.get("embedding")
        if vec and isinstance(vec, list) and len(vec) == 3072:
            valid_data.append({
                "id": doc["id"],
                "text": doc["text"],
                "vector": vec,
                "source": doc["source"]
            })
        else:
            print(f"‚ö†Ô∏è Skipping invalid vector for doc ID {doc['id']}")

    print(f"üìä Vectors to insert: {len(valid_data)}")

    if not valid_data:
        print("‚ùå No valid embeddings to insert.")
        return False

    try:
        insert_result = client.insert(collection_name=COLLECTION_NAME, data=valid_data)
        print("üì• Insert result:", insert_result)

        try:
            client.load_collection(COLLECTION_NAME)
            print("‚úÖ Collection loaded.")
        except Exception as e:
            print(f"‚ùå Collection load failed: {e}")
        return True
    except Exception as e:
        print(f"‚ùå Upsert failed: {e}")
        return False
